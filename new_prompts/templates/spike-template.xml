<?xml version="1.0" encoding="UTF-8"?>
<template>
  <metadata>
    <name>Spike_Template</name>
    <version>1.1</version>
    <source>Generated based on SDLC Artifacts Comprehensive Guideline v1.1 and Context Engineering Framework</source>
    <sdlc_phase>Spike</sdlc_phase>
    <alias>Technical Spike, Investigation Spike, Research Spike</alias>
    <date>2025-10-13</date>
    <changes>v1.1: Path consolidation - Updated link placeholders (Issue #2)</changes>
  </metadata>

  <instructions>
    <guideline>Time-boxed investigation (1-3 days max) to reduce technical uncertainty</guideline>
    <guideline>Focus on answering specific question from Backlog Story or Tech Spec</guideline>
    <guideline>Document findings with data/evidence (benchmarks, code samples, documentation)</guideline>
    <guideline>Provide clear recommendation based on findings</guideline>
    <guideline>Output is research findings and recommendation, NOT production code</guideline>
    <guideline>Link spike results to downstream artifacts (ADR or Tech Spec)</guideline>
  </instructions>

  <structure format="markdown">
    <![CDATA[
# Spike: [Investigation Title]

## Metadata
- **Spike ID:** SPIKE-[XXX]
- **Parent:** [US-XXX: Story Title] or [SPEC-XXX: Spec Title]
- **Status:** [Planned/In Progress/Completed]
- **Assigned To:** [Developer Name]
- **Sprint:** [Sprint 15]
- **Created Date:** [YYYY-MM-DD]
- **Completed Date:** [YYYY-MM-DD or N/A]

## Investigation Goal

### Question to Answer
[Original question from Backlog Story Open Questions marked as [REQUIRES SPIKE]]

**Source Reference:** {SDLC_DOCUMENTS_URL}/backlog-story/{id} (Open Questions section) OR {SDLC_DOCUMENTS_URL}/tech-spec/{id} (section)

### Success Criteria
[How will we know we have enough information to make a decision?]
- [Criterion 1: e.g., "Benchmark data showing query performance under 200ms"]
- [Criterion 2: e.g., "Clear understanding of integration complexity"]
- [Criterion 3: e.g., "Documentation of trade-offs between approaches"]

### Out of Scope
[What we explicitly will NOT investigate - keep spike focused]
- [Out of scope item 1]
- [Out of scope item 2]

## Time Box & Schedule

**Time Box:** [1-3 days maximum]
- **Planned Start:** [Date]
- **Planned End:** [Date]
- **Actual Hours Spent:** [To be filled on completion]

**Time Box Enforcement:**
If investigation is not complete within time box, stop and document findings with current state. Extend time box only if approved by Tech Lead.

## Investigation Approach

**Methods:**
- [Method 1: e.g., "Prototype Redis caching with sample dataset"]
- [Method 2: e.g., "Benchmark query performance with different index strategies"]
- [Method 3: e.g., "Review documentation for third-party API integration"]
- [Method 4: e.g., "Code review of similar implementations in codebase"]

**Environment:**
- [Development environment, staging, local machine, etc.]
- [Tools needed: benchmarking tools, profiling tools, etc.]

**Data Sources:**
- [Sample data or production-like data to use]
- [Documentation sources to review]
- [Code repositories to examine]

## Findings

### Summary
[2-3 sentence summary of what was learned]

### Detailed Findings

#### Finding 1: [Finding Title]
**What we learned:**
[Description of finding]

**Evidence:**
[Benchmarks, code samples, documentation references, screenshots]

**Implications:**
[What this means for implementation]

#### Finding 2: [Finding Title]
**What we learned:**
[Description of finding]

**Evidence:**
[Benchmarks, code samples, documentation references]

**Implications:**
[What this means for implementation]

[Repeat for additional findings]

### Data Collected

**Benchmarks (if applicable):**
| Scenario | Metric | Result | Notes |
|----------|--------|--------|-------|
| [Scenario 1] | [Response time] | [150ms] | [Under load] |
| [Scenario 2] | [Throughput] | [1000 req/s] | [Peak] |

**Code Samples:**
```[language]
// Sample code demonstrating approach
[Code example from prototype or existing codebase]
```

**Documentation References:**
- [Link to documentation 1 with key takeaways]
- [Link to documentation 2 with key takeaways]

### Challenges Encountered
[Any obstacles, blockers, or unexpected issues during investigation]
- [Challenge 1 and how it was addressed]
- [Challenge 2 and how it was addressed]

### Unknowns Remaining
[Questions that still cannot be answered even after spike]
- [Unknown 1 - may need additional spike or accept uncertainty]
- [Unknown 2 - may need Tech Lead decision]

## Recommendation

**Recommended Approach:** [Clear statement of what should be done]

**Rationale:**
[Why this approach is recommended based on findings]
- [Reason 1 with data from findings]
- [Reason 2 with data from findings]
- [Reason 3 with data from findings]

**Alternative Approaches Considered:**
- **Alternative 1:** [Brief description]
  - **Why not chosen:** [Reason based on findings]
- **Alternative 2:** [Brief description]
  - **Why not chosen:** [Reason based on findings]

**Risk Assessment:**
- **Low/Medium/High Risk**
- [Risk factor 1]
- [Risk factor 2]

**Confidence Level:**
- **High/Medium/Low Confidence** in recommendation
- [Factors affecting confidence]

## Implementation Notes

**If Recommendation Accepted:**
- [Implementation consideration 1]
- [Implementation consideration 2]
- [Estimated complexity: Low/Medium/High]
- [Estimated effort: X story points or days]

**Dependencies:**
- [Dependency 1: Libraries, tools, services needed]
- [Dependency 2]

**Migration Path (if applicable):**
- [Step 1]
- [Step 2]

## Next Steps

### Immediate Actions
- [ ] Update Parent Story [{SDLC_DOCUMENTS_URL}/backlog-story/{id}] with spike findings and recommendation
- [ ] Update story estimate based on findings
- [ ] Decide if ADR needed (if significant architectural decision)
- [ ] Decide if Tech Spec update needed (if implementation details clarified)

### Downstream Artifacts to Create/Update
- [ ] **ADR:** [{SDLC_DOCUMENTS_URL}/adr/{id} - if significant decision with alternatives analysis needed]
- [ ] **Tech Spec:** [Update {SDLC_DOCUMENTS_URL}/tech-spec/{id} with implementation details]
- [ ] **Backlog Story:** [Update {SDLC_DOCUMENTS_URL}/backlog-story/{id} technical notes section]

### Follow-up Questions for Team
- [Question 1 for team discussion]
- [Question 2 for Tech Lead input]

## Traceability

**Triggered By:**
- **Backlog Story:** {SDLC_DOCUMENTS_URL}/backlog-story/{id}
- **Open Question:** [Original question from US-XXX]

**Informs:**
- **ADR:** {SDLC_DOCUMENTS_URL}/adr/{id} (if created)
- **Tech Spec:** {SDLC_DOCUMENTS_URL}/tech-spec/{id} (if updated)
- **Backlog Story:** {SDLC_DOCUMENTS_URL}/backlog-story/{id} (updated with findings)

**Related Spikes:**
- {SDLC_DOCUMENTS_URL}/spike/{id}: [Related investigation]

## References

**Documentation:**
- [Link to external documentation 1]
- [Link to external documentation 2]

**Code Samples:**
- [Link to prototype repository or branch]
- [Link to similar implementation in codebase]

**Benchmark Results:**
- [Link to benchmark data or spreadsheet]
- [Link to profiling results]

**Discussion Notes:**
- [Link to Slack thread or meeting notes]
    ]]>
  </structure>

  <examples>
    <example>
      # Spike: Evaluate Redis for Notification Preferences Caching

      ## Metadata
      - **Spike ID:** SPIKE-042
      - **Parent Story:** US-001-003: Update User Notification Preferences API
      - **Status:** Complete
      - **Assigned To:** Jane Doe
      - **Sprint:** Sprint 12
      - **Completed Date:** 2025-10-15

      ## Investigation Goal

      ### Question to Answer
      "Does Redis caching improve preference lookup performance enough to justify the added complexity?"

      **Source Reference:** US-001-003 Open Questions, Question 2

      ### Success Criteria
      - Benchmark data comparing PostgreSQL-only vs Redis caching
      - Understanding of Redis integration complexity
      - Clear cost/benefit analysis

      ## Time Box &amp; Schedule
      **Time Box:** 2 days
      **Actual Hours Spent:** 12 hours

      ## Investigation Approach
      **Methods:**
      - Created prototype with Redis caching layer
      - Benchmarked query performance under various load scenarios
      - Reviewed Redis client documentation and best practices

      ## Findings

      ### Summary
      Redis caching provides 10x performance improvement for read-heavy preference lookups but adds moderate complexity. Recommended for production use given high read:write ratio (95:5).

      ### Detailed Findings

      #### Finding 1: Performance Improvement
      **What we learned:**
      Redis caching reduces p99 lookup latency from 180ms to 18ms under load.

      **Evidence:**
      | Scenario | PostgreSQL Only | With Redis | Improvement |
      |----------|-----------------|------------|-------------|
      | 100 concurrent reads | 180ms p99 | 18ms p99 | 10x faster |
      | 1000 concurrent reads | 450ms p99 | 22ms p99 | 20x faster |

      **Implications:**
      Meets PRD requirement of &lt;200ms p99 even under peak load.

      #### Finding 2: Integration Complexity
      **What we learned:**
      Redis integration requires cache invalidation strategy and ~200 LOC.

      **Evidence:**
      - Reviewed `ioredis` client documentation
      - Prototype implementation: 180 LOC for caching layer + 20 LOC for invalidation

      **Implications:**
      Moderate complexity, but manageable with existing team expertise.

      ## Recommendation

      **Recommended Approach:** Implement Redis caching for notification preferences with write-through invalidation strategy.

      **Rationale:**
      - 10x performance improvement justifies added complexity
      - High read:write ratio (95:5) ideal for caching
      - Meets PRD performance requirements under peak load
      - Team has Redis expertise from previous projects

      **Confidence Level:** High

      ## Next Steps
      - [x] Update US-001-003 with findings
      - [x] Update story estimate from 5 SP to 8 SP (added Redis complexity)
      - [ ] Create ADR-008: Redis for Notification Preferences Caching
      - [ ] Update Tech Spec with Redis implementation details
    </example>
  </examples>
</template>
