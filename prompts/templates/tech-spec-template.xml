<?xml version="1.0" encoding="UTF-8"?>
<template>
  <metadata>
    <name>Technical_Specification_Template</name>
    <version>1.1</version>
    <source>Section 6.3 of advanced_prompt_engineering_software_docs_code_final.md (lines 905-1016)</source>
    <sdlc_phase>Technical_Design</sdlc_phase>
    <last_updated>2025-10-11</last_updated>
    <changes>v1.1: Enhanced Open Questions section with comprehensive guidance on implementation-level questions, clear boundaries with PRD/ADR phases, and examples</changes>
  </metadata>

  <instructions>
    <guideline>Create implementation specifications for engineering teams</guideline>
    <guideline>Include detailed API specifications with request/response examples</guideline>
    <guideline>Define data models with clear interfaces</guideline>
    <guideline>Specify security, performance, and testing requirements</guideline>
    <guideline>Provide implementation plan with phases</guideline>
  </instructions>

  <structure format="markdown">
    <![CDATA[
# Technical Specification: [Feature/System Name]

## Metadata
- **TechSpec ID:** SPEC-[XXX]
- **Date:** [YYYY-MM-DD]
- **Status:** [Proposed | Accepted | Deprecated | Superseded]
- **Deciders:** [Names]
- **Parent Story ID:** US-[XXX]
- **Related PRD:** [Link]
- **Related ADR:** [Link]
- **Informed By Spike:** SPIKE-[XXX] (if spike provided implementation guidance)
- **Informed By Implementation Research:** [Link to Implementation Research document]

## Overview
**Summary:** [One paragraph overview]
**Related PRD:** [Link]
**Related ADR:** [Link]

## Research & Investigation Context

**Parent Backlog Story:** [US-XXX: Story Title]
- **Link:** [URL to Backlog Story]

**Informed By Spike:** [SPIKE-XXX: Spike Title] (if applicable)
- **Link:** [URL to Spike document]
- **Implementation Guidance:** [How spike findings shaped this technical spec]
- **Key Decisions:** [Decisions made based on spike evidence]

**Implementation Research References:**
**Primary Research Document:** [Link to Implementation Research report]

**Implementation Patterns Applied:**
- **§[X.Y]: [Technical Capability]:** [Detailed implementation pattern]
  - **Code Example Reference:** [Link to Appendix B example if applicable]
- **§[X.Y]: [Security Implementation]:** [Security pattern applied]
- **§[X.Y]: [Testing Strategy]:** [Testing approach from research]

**Performance Targets:**
- **§[X.Y]: [Performance Benchmarks]:** [Target metrics from research]

**Observability:**
- **§[X.Y]: [Monitoring Implementation]:** [Logging and metrics approach]

## Goals
- [Technical goal 1]
- [Technical goal 2]

## Non-Goals
- [Explicitly out of scope]

## System Architecture

### High-Level Architecture
```mermaid
graph TD
    A[Client] --> B[API Gateway]
    B --> C[Service Layer]
    C --> D[Data Layer]
```

### Component Diagram
[Detailed component interactions]

## API Specifications

### Endpoint: [Name]
**Method:** POST
**Path:** `/api/v1/resource`

**Request:**
```json
{
  "field1": "string",
  "field2": "number"
}
```

**Response:**
```json
{
  "id": "uuid",
  "status": "success"
}
```

**Error Codes:**
- 400: Invalid input
- 401: Unauthorized
- 500: Server error

## Data Models

### Entity: User
```typescript
interface User {
  id: UUID;
  email: string;
  createdAt: DateTime;
  // ...
}
```

## Security Considerations
- **Authentication:** [Method]
- **Authorization:** [RBAC model]
- **Data Encryption:** [At-rest, in-transit]
- **Input Validation:** [Strategy]

## Performance Requirements
- **Latency:** P95 < 200ms
- **Throughput:** 10,000 requests/sec
- **Availability:** 99.9% uptime

## Testing Strategy
- **Unit Tests:** [Coverage target: 80%]
- **Integration Tests:** [Critical paths]
- **Load Tests:** [Scenarios]

## Deployment Strategy
- **Environment Progression:** Dev → Staging → Production
- **Rollout Plan:** [Phased rollout strategy]
- **Rollback Plan:** [Procedure]

## Monitoring & Observability
- **Key Metrics:** [List]
- **Alerts:** [Conditions and escalation]
- **Logging:** [Strategy and retention]

## Implementation Plan
### Phase 1: [Duration]
- [Task 1]
- [Task 2]

### Phase 2: [Duration]
- [Task 3]
- [Task 4]

## Open Questions

**Tech Spec Open Questions focus on IMPLEMENTATION-LEVEL details that require resolution during development.**

**✅ INCLUDE - Implementation Details:**
- Low-level technology/library choices not covered by ADRs
- Implementation approach questions for specific components
- Performance optimization strategies
- Testing approach details
- Component-level design pattern choices
- Minor technical decisions requiring team discussion or spike

**❌ EXCLUDE - Already Addressed in Earlier Phases:**
- Product/technical trade-offs (PRD phase)
- Major architectural decisions requiring alternatives analysis (ADR phase)
- Business questions (Epic/PRD phases)
- Significant technology choices with major consequences (ADR phase)

---

**Examples of Tech Spec-APPROPRIATE questions:**
- "Should we use Joi or Yup for request validation?"
- "What pagination strategy for the user list endpoint: cursor or offset?"
- "Should we implement retry logic with exponential backoff or fixed delay?"
- "What's the optimal batch size for bulk operations?"
- "Should we use async/await or promise chaining for this workflow?"

**Examples of questions already addressed (don't repeat):**
- ❌ "Should we use Redis or Memcached for caching?" (ADR decision)
- ❌ "Should we prioritize offline-first or real-time sync?" (PRD decision)
- ❌ "REST vs. GraphQL for API layer?" (ADR decision)

---

**If no open questions exist, state:** "No open implementation questions at this time. All technical decisions addressed in PRD and ADRs."

---

- [Implementation Question 1]
- [Implementation Question 2]

## References
- [Documentation links]
    ]]>
  </structure>

  <examples>
    <example source="Section 6.3 lines 1020-1046">
      See research document for User Profile API specification example with:
      - OpenAPI 3.0 format
      - JWT authentication
      - CRUD operations
      - Error handling
    </example>
  </examples>
</template>
